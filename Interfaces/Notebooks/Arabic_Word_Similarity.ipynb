{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic_Word_Similarity.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Q9I1X5jewT",
        "colab_type": "code",
        "outputId": "7013e683-8358-49f1-ef15-c356ed0501d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pyarabic"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.6/dist-packages (0.6.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcYYM-STZE_b",
        "colab_type": "code",
        "outputId": "5fa98adb-7f41-48c2-b292-bd057d6deebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Reshape, merge, Multiply\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "\n",
        "import urllib.request\n",
        "import collections\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pyarabic import araby"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq_qnuXXIn5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "57cde936-a10c-476a-e4c1-76b17fb8d5fe"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/zaidalyafeai/ARBML/master/datasets/Wiki/wiki_ar.txt\n",
        "wiki_path = 'wiki_ar.txt'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-19 10:09:00--  https://raw.githubusercontent.com/zaidalyafeai/ARBML/master/datasets/Wiki/wiki_ar.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59054425 (56M) [text/plain]\n",
            "Saving to: ‘wiki_ar.txt.1’\n",
            "\n",
            "wiki_ar.txt.1       100%[===================>]  56.32M   214MB/s    in 0.3s    \n",
            "\n",
            "2019-06-19 10:09:01 (214 MB/s) - ‘wiki_ar.txt.1’ saved [59054425/59054425]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVQHw9WYMlrr",
        "colab_type": "text"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVJEguNCMeoR",
        "colab_type": "text"
      },
      "source": [
        "Remove stope words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzbZzX_ZOiSv",
        "colab_type": "code",
        "outputId": "56a0ce26-7c31-4e83-9b55-19475a3c6899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "\n",
        "remove_as_well = set(['ان', 'او', 'ا', 'لا', 'في', 'على', 'الى', 'اي', 'م', 'تكون', 'كان', 'من', 'اذا', 'مع'])\n",
        "arb_stopwords |= remove_as_well"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0eUHhByMjqM",
        "colab_type": "text"
      },
      "source": [
        "## Process the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaDusgg2ZL8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the data into a list of strings.\n",
        "def read_data(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "      corpus = f.read()\n",
        "      corpus = corpus.replace('\\n',' ')\n",
        "      corpus = araby.strip_tashkeel(corpus)\n",
        "    return corpus.split()\n",
        "\n",
        "\n",
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        if word not in arb_stopwords:\n",
        "          dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            index = dictionary[word]\n",
        "        else:\n",
        "            index = 0  # dictionary['UNK']\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary\n",
        "\n",
        "def collect_data(vocabulary_size=10000):\n",
        "    vocabulary = read_data(wiki_path)\n",
        "    print(vocabulary[:7])\n",
        "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
        "                                                                vocabulary_size)\n",
        "    del vocabulary  # Hint to reduce memory.\n",
        "    return data, count, dictionary, reverse_dictionary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXLicqWoMpVd",
        "colab_type": "text"
      },
      "source": [
        "## Create the dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO1T1w6n3cJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
        "print(data[:7])\n",
        "print(len(data))\n",
        "vocab_size = len(dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBccbW1eM56L",
        "colab_type": "text"
      },
      "source": [
        "## Sample 3-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvncCrqGZjym",
        "colab_type": "code",
        "outputId": "ddcd1ca3-3d3a-435e-8b7e-abc867af90a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#setup parameters\n",
        "window_size = 3\n",
        "vector_dim = 256\n",
        "\n",
        "#sample data according to the window size \n",
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "\n",
        "word_target = np.array(word_target, dtype=\"int32\")\n",
        "word_context = np.array(word_context, dtype=\"int32\")\n",
        "\n",
        "print(couples[:10], labels[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[829, 4160], [20, 587], [9363, 1378], [1145, 2296], [343, 4073], [8536, 7311], [7425, 195], [3932, 750], [5868, 13], [5203, 6601]] [1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbf9PDatM8ev",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2XxSTo2Z3Oi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "09906372-b2ab-4f36-ae1c-75b98809849c"
      },
      "source": [
        "# create some input variables\n",
        "input_target = Input((1,))\n",
        "input_context = Input((1,))\n",
        "\n",
        "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
        "target = embedding(input_target)\n",
        "target = Reshape((vector_dim, 1))(target)\n",
        "context = embedding(input_context)\n",
        "context = Reshape((vector_dim, 1))(context)\n",
        "\n",
        "# setup a cosine similarity operation which will be output in a secondary model\n",
        "similarity = keras.layers.Dot(axes = 0, normalize = True)([target, context])\n",
        "\n",
        "# now perform the dot product operation to get a similarity measure\n",
        "dot_product = keras.layers.Dot(axes = 1)([target, context])\n",
        "\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "# add the sigmoid output layer\n",
        "output = Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "# create the primary training model\n",
        "model = Model(inputs=[input_target, input_context], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# create a secondary validation model to run our similarity checks during training\n",
        "validation_model = Model(inputs=[input_target, input_context], outputs=similarity)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 10:12:38.149486 139925669316480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0619 10:12:38.164864 139925669316480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0619 10:12:38.169638 139925669316480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0619 10:12:38.222889 139925669316480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0619 10:12:38.242461 139925669316480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0619 10:12:38.247903 139925669316480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmESkb7WZQH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_examples = np.array([dictionary[text] for text in ['ابريل', 'المانيا', 'خمسة']])\n",
        "valid_size = len(valid_examples)\n",
        "\n",
        "class SimilarityCallback:\n",
        "    def run_sim(self):\n",
        "        for i in range(valid_size):\n",
        "            valid_word = reverse_dictionary[valid_examples[i]]\n",
        "            top_k = 8  # number of nearest neighbors\n",
        "            sim = self._get_sim(valid_examples[i])\n",
        "            nearest = (-sim).argsort()[1:top_k + 1]\n",
        "            log_str = 'Nearest to %s:' % valid_word\n",
        "            for k in range(top_k):\n",
        "                close_word = reverse_dictionary[nearest[k]]\n",
        "                log_str = '%s %s,' % (log_str, close_word)\n",
        "            print(log_str)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_sim(valid_word_idx):\n",
        "        sim = np.zeros((vocab_size,))\n",
        "        in_arr1 = np.zeros((1,))\n",
        "        in_arr2 = np.zeros((1,))\n",
        "        in_arr1[0,] = valid_word_idx\n",
        "        for i in range(vocab_size):\n",
        "            in_arr2[0,] = i\n",
        "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
        "            sim[i] = out\n",
        "        return sim\n",
        "sim_cb = SimilarityCallback()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZSQmUeM__b",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC5jlxeAnGOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(idx, word_target, word_context, labels, batch_size = 32):\n",
        "  arr_1 = word_target[idx*batch_size: (idx + 1)* batch_size]\n",
        "  arr_2 = word_context[idx*batch_size: (idx + 1)* batch_size]\n",
        "  arr_3 = labels[idx*batch_size: (idx + 1)* batch_size]\n",
        "  return arr_1, arr_2, arr_3 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqsfSpNIZypI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "a4ffb368-eb5f-4ded-94b7-090ddada0e47"
      },
      "source": [
        "batch_size = 256\n",
        "avg_loss = 0.0\n",
        "epochs = 1\n",
        "for cnt in range(epochs):\n",
        "    num_batches = len(labels)// batch_size\n",
        "    \n",
        "    for idx in range(0, num_batches):\n",
        "      batch_idx = np.random.randint(0, num_batches)\n",
        "      arr_1, arr_2, arr_3  = get_batch(batch_idx, word_target, word_context, labels, batch_size = batch_size)\n",
        "      loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
        "      avg_loss += loss\n",
        "            \n",
        "      if idx % 1000 == 0 and idx != 0:\n",
        "        print(\"Iteration {}, loss={}\".format(cnt, avg_loss/1000))\n",
        "        avg_loss = 0.0\n",
        "      if idx % 10000 == 0:\n",
        "        sim_cb.run_sim()\n",
        "        print(\" \")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 10:12:57.568955 139925669316480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Nearest to ابريل: قلت, اتفاقيات, الشعرية, القطري, عرضة, العناية, العاملين, تتوفر,\n",
            "Nearest to المانيا: مبكرة, يملكون, مجزرة, المستويات, تصميم, الفقري, مروان, الماعز,\n",
            "Nearest to خمسة: الثوري, البلاشفة, تمنح, التركيز, انها, التقليد, قابلية, عائشة,\n",
            " \n",
            "Iteration 0, loss=0.6854368621706962\n",
            "Iteration 0, loss=0.6308442072570324\n",
            "Iteration 0, loss=0.5786757318675518\n",
            "Iteration 0, loss=0.5433163019865751\n",
            "Iteration 0, loss=0.5185977192521095\n",
            "Iteration 0, loss=0.49843697002530096\n",
            "Iteration 0, loss=0.48290011164546015\n",
            "Iteration 0, loss=0.4693636109381914\n",
            "Iteration 0, loss=0.45512205351144075\n",
            "Iteration 0, loss=0.4394383922591805\n",
            "Nearest to ابريل: عام, العام, مايو, خلال, فبراير, بروسيا, اكتوبر, الاولى,\n",
            "Nearest to المانيا: البلاد, كانت, معظم, عام, شعوب, و, مسلمين, خسارة,\n",
            "Nearest to خمسة: تسعة, المسافرين, حوالي, و, مدينة, اربعة, مسجدا, ثلاث,\n",
            " \n",
            "Iteration 0, loss=0.4312197460308671\n",
            "Iteration 0, loss=0.4162627774998546\n",
            "Iteration 0, loss=0.4210812357440591\n",
            "Iteration 0, loss=0.4116187410354614\n",
            "Iteration 0, loss=0.39525401350855827\n",
            "Iteration 0, loss=0.39542841275036333\n",
            "Iteration 0, loss=0.3814113288149238\n",
            "Iteration 0, loss=0.3827954874113202\n",
            "Iteration 0, loss=0.36900510571897027\n",
            "Iteration 0, loss=0.36805023621022703\n",
            "Nearest to ابريل: اكتوبر, خلال, فرنسا, بتاريخ, نيسان, يوليو, موانئ, فبراير,\n",
            "Nearest to المانيا: عام, اللغة, العالمية, كبرى, الاسرائيلية, جميع, الاخير, العاصمة,\n",
            "Nearest to خمسة: الدولة, اليابانيين, ثلاث, عدة, عاما, مجال, قبل, عطارد,\n",
            " \n",
            "Iteration 0, loss=0.3604791887961328\n",
            "Iteration 0, loss=0.35505277847498656\n",
            "Iteration 0, loss=0.3591612151414156\n",
            "Iteration 0, loss=0.35462877998873593\n",
            "Iteration 0, loss=0.3431920439228415\n",
            "Iteration 0, loss=0.34217240975797175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oij1f1oPNCSr",
        "colab_type": "text"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPx8Ppfsu5pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k(word, top_k = 8):\n",
        "    valid_word_idx = dictionary[word]\n",
        "    sim = get_similar(valid_word_idx)\n",
        "    nearest = (-sim).argsort()[1:top_k + 1]\n",
        "    log_str = word + ':'\n",
        "    for k in range(top_k):\n",
        "        close_word = reverse_dictionary[nearest[k]]\n",
        "        log_str = '%s  %s -' % (log_str, close_word)\n",
        "    print(log_str)\n",
        "\n",
        "def get_similar(valid_word_idx):\n",
        "    sim = np.zeros((vocab_size,))\n",
        "    in_arr1 = np.zeros((1,))\n",
        "    in_arr2 = np.zeros((1,))\n",
        "    in_arr1 = np.array([valid_word_idx]*vocab_size)\n",
        "    in_arr2 = np.array(range(vocab_size))\n",
        "    out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
        "    out = out.reshape((vocab_size,))\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1PKOhwRR7iZ",
        "colab_type": "code",
        "outputId": "7520803a-5353-406d-9a16-2146a286bebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "find_top_k('سبعة')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "سبعة:  ثمانية -  خمسة -  آلاف -  سنة -  اثني -  ثلاثة -  اربعة -  الفا -\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S8VcDS4NGTr",
        "colab_type": "text"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cirEUugU3gpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_model.save('keras.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZGrppss34Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the dictionary\n",
        "import csv\n",
        "def create_csv(file, dict):\n",
        "    with open(file, 'w') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        for key in dict.keys():\n",
        "            writer.writerow([key,dict[key]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMCPgJyD5rzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_csv('idx2word.csv', reverse_dictionary)\n",
        "create_csv('word2idx.csv', dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-jy4ru6MQZL",
        "colab_type": "text"
      },
      "source": [
        "https://adventuresinmachinelearning.com/word2vec-keras-tutorial/"
      ]
    }
  ]
}