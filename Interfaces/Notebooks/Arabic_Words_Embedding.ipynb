{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic Words Embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cjXpwiymRU9",
        "colab_type": "code",
        "outputId": "20177760-3660-4208-ed87-0bc25148208e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget https://archive.org/download/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip\n",
        "!unzip full_grams_cbow_300_twitter.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-28 18:00:09--  https://archive.org/download/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia802903.us.archive.org/26/items/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip [following]\n",
            "--2019-05-28 18:00:11--  https://ia802903.us.archive.org/26/items/full_grams_cbow_300_twitter/full_grams_cbow_300_twitter.zip\n",
            "Resolving ia802903.us.archive.org (ia802903.us.archive.org)... 207.241.233.53\n",
            "Connecting to ia802903.us.archive.org (ia802903.us.archive.org)|207.241.233.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3325529808 (3.1G) [application/zip]\n",
            "Saving to: ‘full_grams_cbow_300_twitter.zip’\n",
            "\n",
            "full_grams_cbow_300 100%[===================>]   3.10G  2.56MB/s    in 29m 0s  \n",
            "\n",
            "2019-05-28 18:29:11 (1.82 MB/s) - ‘full_grams_cbow_300_twitter.zip’ saved [3325529808/3325529808]\n",
            "\n",
            "Archive:  full_grams_cbow_300_twitter.zip\n",
            "  inflating: full_grams_cbow_300_twitter.mdl  \n",
            "  inflating: full_grams_cbow_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_cbow_300_twitter.mdl.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlrmsR2bm4Pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "\n",
        "# =========================\n",
        "# ==== Helper Methods =====\n",
        "\n",
        "# Clean/Normalize Arabic Text\n",
        "def clean_str(text):\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "    \n",
        "    #remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel,\"\", text)\n",
        "    \n",
        "    #remove longation\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "    \n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    \n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "    \n",
        "    #trim    \n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_vec(n_model,dim, token):\n",
        "    vec = np.zeros(dim)\n",
        "    is_vec = False\n",
        "    if token not in n_model.wv:\n",
        "        _count = 0\n",
        "        is_vec = True\n",
        "        for w in token.split(\"_\"):\n",
        "            if w in n_model.wv:\n",
        "                _count += 1\n",
        "                vec += n_model.wv[w]\n",
        "        if _count > 0:\n",
        "            vec = vec / _count\n",
        "    else:\n",
        "        vec = n_model.wv[token]\n",
        "    return vec\n",
        "\n",
        "def calc_vec(pos_tokens, neg_tokens, n_model, dim):\n",
        "    vec = np.zeros(dim)\n",
        "    for p in pos_tokens:\n",
        "        vec += get_vec(n_model,dim,p)\n",
        "    for n in neg_tokens:\n",
        "        vec -= get_vec(n_model,dim,n)\n",
        "    \n",
        "    return vec   \n",
        "\n",
        "## -- Retrieve all ngrams for a text in between a specific range\n",
        "def get_all_ngrams(text, nrange=3):\n",
        "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
        "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
        "    ngs = []\n",
        "    for n in range(2,nrange+1):\n",
        "        ngs += [ng for ng in ngrams(tokens, n)]\n",
        "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
        "\n",
        "## -- Retrieve all ngrams for a text in a specific n\n",
        "def get_ngrams(text, n=2):\n",
        "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
        "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
        "    ngs = [ng for ng in ngrams(tokens, n)]\n",
        "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
        "\n",
        "## -- filter the existed tokens in a specific model\n",
        "def get_existed_tokens(tokens, n_model):\n",
        "    return [tok for tok in tokens if tok in n_model.wv ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlMxaNY-ljVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ============================   \n",
        "# ====== N-Grams Models ======\n",
        "\n",
        "t_model = gensim.models.Word2Vec.load('full_grams_cbow_300_twitter.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioxbp5PUm2rm",
        "colab_type": "code",
        "outputId": "a3f76242-c3cb-4c19-c933-359181030070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# python 3.X\n",
        "token = clean_str(u'بطاطس').replace(\" \", \"_\")\n",
        "\n",
        "if token in t_model.wv:\n",
        "    most_similar = t_model.wv.most_similar( token, topn=10 )\n",
        "    for term, score in most_similar:\n",
        "        term = clean_str(term).replace(\" \", \"_\")\n",
        "        if term != token:\n",
        "            print(term, score)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ايس_كريم 0.7060004472732544\n",
            "شبس 0.6992693543434143\n",
            "اندومي 0.696632981300354\n",
            "بيتزا 0.696553111076355\n",
            "بصل 0.6903796195983887\n",
            "مكرونه 0.6877341270446777\n",
            "جبنه 0.6842865943908691\n",
            "رز 0.680794358253479\n",
            "كنافه 0.6693990230560303\n",
            "زبادي 0.6684207916259766\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}